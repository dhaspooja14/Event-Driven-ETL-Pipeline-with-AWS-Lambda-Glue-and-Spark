Lambda Function
-------------------------------------------------------------------------------------------------------------------------------
import json
import boto3
def lambda_handler(event, context):
    client = boto3.client('glue')
    bucket_name = event['Records'][0]['s3']['bucket']['name']
    object_key = event['Records'][0]['s3']['object']['key']
    fullpath = 's3://{}/{}'.format(bucket_name, object_key)
    print(fullpath)
    print("----------------------------------------")
    response = client.start_job_run(
    JobName='gluecsv',
    Arguments={
        '--buck':bucket_name,
        '--obj':object_key,
        '--s3_path':fullpath
    })
-----------------------------------------------------------------------------------------------------------------------
Glue job
-----------------------------------------------------------------------------------------------------------------------
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import *

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['buck','obj','s3_path'])
bucket=args['buck']
file=args['obj']
data=args['s3_path']
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
import os
import re
filename_with_extension = os.path.basename(data)
filename_without_extension = os.path.splitext(filename_with_extension)[0]
tab=re.sub(r'[^A-Za-z0-9]', '', filename_without_extension)

df = spark.read.format("csv").option("header","true").option("inferSchema","true").load(data)
ndf = df.withColumn("ts",current_timestamp())

myconf = {
    "url":"jdbc:mysql://mysqldb.czegw88igo9k.ap-south-1.rds.amazonaws.com:3306/poojadb",
    "driver":"com.mysql.cj.jdbc.Driver",
    "user":"myuser",
    "password":"mypassword"
}

ndf.write.mode("append").format("jdbc").options(**myconf).option("dbtable",tab).save()
